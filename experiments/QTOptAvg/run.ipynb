{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from typing import Any, List\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.server.history import History\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from florl.common.util import aggregate_weighted_average, stateful_client\n",
    "\n",
    "from qtoptavg import *\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLIENTS = 5\n",
    "TOTAL_ROUNDS = 20\n",
    "FRAMES_PER_ROUND = 25\n",
    "# EXPERIMENT_REPEATS = 30\n",
    "EXPERIMENT_REPEATS = 2\n",
    "\n",
    "config = DictConfig({\n",
    "    \"rl\": {\n",
    "        \"env\": {\n",
    "            \"name\": \"Pendulum-v1\"\n",
    "        },\n",
    "        \"algorithm\": {\n",
    "            \"gamma\": 0.99,\n",
    "            \"tau\": 0.005,\n",
    "            \"lr\": 0.001,\n",
    "            \"update_frequency\": 1,\n",
    "            \"clip_grad_norm\": 1,\n",
    "            \"critic\": {\n",
    "                \"features\": 64\n",
    "            }\n",
    "        },\n",
    "        \"memory\": {\n",
    "            \"type\": \"experience_replay\",\n",
    "            \"capacity\": max(128, TOTAL_ROUNDS * FRAMES_PER_ROUND)\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"initial_collection_size\": 1024,\n",
    "            \"minibatch_size\": 64\n",
    "        }\n",
    "    },\n",
    "    \"fl\": {\n",
    "        \"train_config\": {\n",
    "            \"frames\": FRAMES_PER_ROUND,\n",
    "        },\n",
    "        \"evaluate_config\": {\n",
    "            \"evaluation_repeats\": 1\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "train_config = OmegaConf.to_container(config[\"fl\"][\"train_config\"])\n",
    "evaluate_config = OmegaConf.to_container(config[\"fl\"][\"evaluate_config\"])\n",
    "\n",
    "def _on_fit_config_fn(server_round: int):\n",
    "        return train_config | {\"server_round\": server_round}\n",
    "def _on_evaluate_config_fn(server_round: int):\n",
    "    return evaluate_config | {\"server_round\": server_round}\n",
    "\n",
    "client_factory = QTOptClientFactory(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = []\n",
    "for seed in range(EXPERIMENT_REPEATS):\n",
    "    client = client_factory.create_dqn_client(seed, config[\"rl\"])\n",
    "\n",
    "    # Manually run through the training loop\n",
    "    hist_fit = []\n",
    "    evaluation_reward = []\n",
    "    for simulated_rounds in tqdm(range(TOTAL_ROUNDS)):\n",
    "        _, metrics = client.train(config[\"fl\"][\"train_config\"])\n",
    "        hist_fit.append(metrics)\n",
    "        evaluation_reward.append(client._evaluator.evaluate(client.policy, repeats=config[\"fl\"][\"evaluate_config\"][\"evaluation_repeats\"]))\n",
    "\n",
    "    baseline_results.append((hist_fit, evaluation_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WS = \"florl_ws\"\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "#strategy = fl.server.strategy.FedProx(\n",
    "    #proximal_mu=0.1,\n",
    "    on_fit_config_fn = _on_fit_config_fn,\n",
    "    on_evaluate_config_fn= _on_evaluate_config_fn,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average, \n",
    "    accept_failures=False\n",
    ")\n",
    "\n",
    "federated_results = []\n",
    "\n",
    "for seed in tqdm(range(EXPERIMENT_REPEATS)):\n",
    "    if os.path.exists(CONTEXT_WS):\n",
    "        shutil.rmtree(CONTEXT_WS)\n",
    "\n",
    "    initialized_clients = {}\n",
    "    @stateful_client\n",
    "    def build_client(cid: str) -> fl.client.Client:\n",
    "        print(cid)\n",
    "        if cid not in initialized_clients.keys():\n",
    "            cid = int(cid) + seed * NUM_CLIENTS\n",
    "            initialized_clients[cid] = client_factory.create_dqn_client(cid, config=config[\"rl\"])\n",
    "            return initialized_clients[cid]\n",
    "        else:\n",
    "            print(f\"Client {cid} reused\")\n",
    "            return initialized_clients[cid]\n",
    "\n",
    "    hist = fl.simulation.start_simulation(\n",
    "        client_fn=build_client,\n",
    "        client_resources={'num_cpus': 1},\n",
    "        config=fl.server.ServerConfig(num_rounds=TOTAL_ROUNDS),\n",
    "        num_clients = NUM_CLIENTS,\n",
    "        strategy = strategy\n",
    "    )\n",
    "\n",
    "    federated_results.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_metrics(results: List[History]):\n",
    "    losses = np.array([[x[1]['all'] for x in hist.metrics_distributed_fit[\"loss\"]] for hist in results])\n",
    "    losses = losses.transpose((0,2,1,3)).reshape((EXPERIMENT_REPEATS*NUM_CLIENTS, TOTAL_ROUNDS, 2))[:,:,1]\n",
    "\n",
    "    rewards = np.array([[x[1]['all'] for x in hist.metrics_distributed[\"reward\"]] for hist in results])\n",
    "    rewards = rewards.transpose((0,2,1,3)).reshape((EXPERIMENT_REPEATS*NUM_CLIENTS, TOTAL_ROUNDS, 2))[:,:,1]\n",
    "\n",
    "    return losses, rewards\n",
    "\n",
    "\n",
    "def plot_losses(ax, xs, losses: List[Any], label: str, color: str=\"green\"):\n",
    "    losses_mean = losses.mean(axis=0)\n",
    "    losses_std = losses.std(axis=0)\n",
    "    ax.plot(xs, losses_mean, color=color, label=label)\n",
    "    # for i in range(NUM_CLIENTS):\n",
    "    #     ax.scatter(rounds, federated_losses[i], color=\"g\", alpha=0.3, s=5)\n",
    "    ax.fill_between(\n",
    "        x=xs,\n",
    "        y1=losses_mean-losses_std*1.96,\n",
    "        y2=losses_mean+losses_std*1.96,\n",
    "        alpha=0.2,\n",
    "        color=color\n",
    "    )\n",
    "\n",
    "def plot_rewards(ax, xs, rewards: List[Any], label: str, color: str=\"green\"):\n",
    "    rewards_mean = rewards.mean(axis=0)\n",
    "    rewards_std = rewards.std(axis=0)\n",
    "\n",
    "\n",
    "    ax.plot(xs, rewards_mean, color=color, label=label)\n",
    "    # for i in range(NUM_CLIENTS):\n",
    "    #     ax.scatter(rounds, federated_rewards[i], color=\"g\", alpha=0.3, s=5)\n",
    "    ax.fill_between(\n",
    "        x=xs,\n",
    "        y1=rewards_mean-rewards_std*1.96,\n",
    "        y2=rewards_mean+rewards_std*1.96,\n",
    "        alpha=0.2,\n",
    "        color=\"g\"\n",
    "    )\n",
    "\n",
    "def plot_fed_results(ax_loss, ax_reward, xs, results: List[Any], label: str, color: str=\"green\"):\n",
    "    losses, rewards = get_federated_metrics(results)\n",
    "    plot_losses(ax_loss, xs, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_size_inches(12,5)\n",
    "fig.suptitle(\"QTOpt/QTOptAvg on CartPole\")\n",
    "\n",
    "rounds = list(range(TOTAL_ROUNDS))\n",
    "\n",
    "# Loss & Rewards\n",
    "baseline_losses = np.array([[s['loss'] for s in ex[0]] for ex in baseline_results])\n",
    "baseline_rewards = np.array([ex[1] for ex in baseline_results])\n",
    "federated_losses, federated_rewards = get_federated_metrics(federated_results)\n",
    "\n",
    "ax_losses = axs[0]\n",
    "ax_losses.set_title(\"Training Loss\")\n",
    "ax_losses.set_ylabel(\"Average Loss\")\n",
    "ax_losses.set_xlabel(\"Round\")\n",
    "# ax_losses.set_prop_cycle(color=['red', 'orange', 'yellow', 'green', 'cyan', 'blue', 'violet'])\n",
    "\n",
    "plot_losses(ax=ax_losses,\n",
    "            xs=rounds,\n",
    "            losses=baseline_losses,\n",
    "            label=\"QtOpt Centralised\",\n",
    "            color=\"red\")\n",
    "\n",
    "plot_losses(ax=ax_losses,\n",
    "            xs=rounds,\n",
    "            losses=federated_losses,\n",
    "            label=\"QtOpt FedAvg\",\n",
    "            color=\"green\")\n",
    "\n",
    "# Evaluation Reward\n",
    "ax_rewards = axs[1]\n",
    "ax_rewards.set_title(\"Evaluation Reward\")\n",
    "ax_rewards.set_ylabel(\"Average Episode Reward\")\n",
    "ax_rewards.set_xlabel(\"Round\")\n",
    "\n",
    "plot_rewards(ax=ax_rewards,\n",
    "            xs=rounds,\n",
    "            rewards=baseline_rewards,\n",
    "            label=\"QtOpt Centralised\",\n",
    "            color=\"red\")\n",
    "\n",
    "plot_rewards(ax=ax_rewards,\n",
    "            xs=rounds,\n",
    "            rewards=federated_rewards,\n",
    "            label=\"QtOpt FedAvg\",\n",
    "            color=\"green\")\n",
    "\n",
    "handles, labels = ax_rewards.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
