{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No significant improvements from only training actor\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from florl.common.util import aggregate_weighted_average, stateful_client\n",
    "from florl.client.kitten.td3 import *\n",
    "\n",
    "from strategy import RlFedAvg\n",
    "from visualisation import *\n",
    "from experiment_utils import *\n",
    "\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLIENTS = 5\n",
    "TOTAL_ROUNDS = 150\n",
    "FRAMES_PER_ROUND = 50\n",
    "EXPERIMENT_REPEATS = 10\n",
    "SEED = 0\n",
    "\n",
    "config = DictConfig({\n",
    "    \"rl\": {\n",
    "        \"env\": {\n",
    "            \"name\": \"Pendulum-v1\"\n",
    "        },\n",
    "        \"algorithm\": {\n",
    "            \"gamma\": 0.99,\n",
    "            \"tau\": 0.005,\n",
    "            \"lr\": 0.001,\n",
    "            \"update_frequency\": 1,\n",
    "            \"clip_grad_norm\": 1,\n",
    "            \"critic\": {\n",
    "                \"features\": 64\n",
    "            }\n",
    "        },\n",
    "        \"memory\": {\n",
    "            \"type\": \"experience_replay\",\n",
    "            \"capacity\": max(128, TOTAL_ROUNDS * FRAMES_PER_ROUND)\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"initial_collection_size\": 1024,\n",
    "            \"minibatch_size\": 64\n",
    "        }\n",
    "    },\n",
    "    \"fl\": {\n",
    "        \"train_config\": {\n",
    "            \"frames\": FRAMES_PER_ROUND,\n",
    "        },\n",
    "        \"evaluate_config\": {\n",
    "            \"evaluation_repeats\": 1\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "train_config = OmegaConf.to_container(config[\"fl\"][\"train_config\"])\n",
    "evaluate_config = OmegaConf.to_container(config[\"fl\"][\"evaluate_config\"])\n",
    "\n",
    "client_factory = TD3ClientFactory(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WS = \"florl_ws\"\n",
    "\n",
    "def _on_fit_config_fn(server_round: int):\n",
    "        return train_config | {\"server_round\": server_round}\n",
    "def _on_evaluate_config_fn(server_round: int):\n",
    "    return evaluate_config | {\"server_round\": server_round}\n",
    "\n",
    "strategy = RlFedAvg(\n",
    "    knowledge=copy.deepcopy(client_factory.create_default_knowledge(config=config[\"rl\"])),\n",
    "    on_fit_config_fn = _on_fit_config_fn,\n",
    "    on_evaluate_config_fn= _on_evaluate_config_fn,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_fn=get_evaluation_fn(client_factory.create_client(0, config[\"rl\"])),\n",
    "    accept_failures=False,\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "federated_results = []\n",
    "rng = np.random.default_rng(seed=SEED)\n",
    "\n",
    "for _ in tqdm(range(EXPERIMENT_REPEATS)):\n",
    "    seed = rng.integers(0, 65535)\n",
    "    if os.path.exists(CONTEXT_WS):\n",
    "        shutil.rmtree(CONTEXT_WS)\n",
    "\n",
    "    initialized_clients = {}\n",
    "\n",
    "    @stateful_client\n",
    "    def build_client(cid: str) -> fl.client.Client:\n",
    "        cid = int(cid) + seed\n",
    "        if cid not in initialized_clients.keys():\n",
    "            initialized_clients[cid] = client_factory.create_client(\n",
    "                cid=cid,\n",
    "                config=config[\"rl\"],\n",
    "                enable_evaluation = False\n",
    "            )\n",
    "            return initialized_clients[cid]\n",
    "        else:\n",
    "            return initialized_clients[cid]\n",
    "\n",
    "    hist = fl.simulation.start_simulation(\n",
    "        client_fn=build_client,\n",
    "        client_resources={'num_cpus': 1},\n",
    "        config=fl.server.ServerConfig(num_rounds=TOTAL_ROUNDS),\n",
    "        num_clients = NUM_CLIENTS,\n",
    "        strategy = strategy\n",
    "    )\n",
    "\n",
    "    federated_results.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WS = \"florl_ws\"\n",
    "\n",
    "def _on_fit_config_fn(server_round: int):\n",
    "        return train_config | {\"server_round\": server_round, \"shards\": \"actor|actor_target\"}\n",
    "def _on_evaluate_config_fn(server_round: int):\n",
    "    return evaluate_config | {\"server_round\": server_round, \"shards\": \"actor|actor_target\"}\n",
    "\n",
    "strategy = RlFedAvg(\n",
    "    knowledge=copy.deepcopy(client_factory.create_default_knowledge(config=config[\"rl\"])),\n",
    "    on_fit_config_fn = _on_fit_config_fn,\n",
    "    on_evaluate_config_fn= _on_evaluate_config_fn,\n",
    "    fit_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_weighted_average,\n",
    "    evaluate_fn=get_evaluation_fn(client_factory.create_client(0, config[\"rl\"])),\n",
    "    accept_failures=False,\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "federated_results_2= []\n",
    "rng = np.random.default_rng(seed=SEED)\n",
    "\n",
    "for _ in tqdm(range(EXPERIMENT_REPEATS)):\n",
    "    seed = rng.integers(0, 65535)\n",
    "    if os.path.exists(CONTEXT_WS):\n",
    "        shutil.rmtree(CONTEXT_WS)\n",
    "\n",
    "    initialized_clients = {}\n",
    "\n",
    "    @stateful_client\n",
    "    def build_client(cid: str) -> fl.client.Client:\n",
    "        cid = int(cid) + seed\n",
    "        if cid not in initialized_clients.keys():\n",
    "            initialized_clients[cid] = client_factory.create_client(\n",
    "                cid=cid,\n",
    "                config=config[\"rl\"],\n",
    "                enable_evaluation = False\n",
    "            )\n",
    "            return initialized_clients[cid]\n",
    "        else:\n",
    "            return initialized_clients[cid]\n",
    "\n",
    "    hist = fl.simulation.start_simulation(\n",
    "        client_fn=build_client,\n",
    "        client_resources={'num_cpus': 1},\n",
    "        config=fl.server.ServerConfig(num_rounds=TOTAL_ROUNDS),\n",
    "        num_clients = NUM_CLIENTS,\n",
    "        strategy = strategy\n",
    "    )\n",
    "\n",
    "    federated_results_2.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_size_inches(12,5)\n",
    "fig.suptitle(\"TD3Avg / TD3Avg (Actor) on Pendulum\")\n",
    "\n",
    "rounds = list(range(TOTAL_ROUNDS))\n",
    "\n",
    "# Loss & Rewards\n",
    "federated_losses, federated_rewards = get_federated_metrics(\n",
    "    federated_results,\n",
    "    EXPERIMENT_REPEATS,\n",
    "    NUM_CLIENTS,\n",
    "    TOTAL_ROUNDS,\n",
    "    centralised_evaluation=True\n",
    ")\n",
    "federated_losses_2, federated_rewards_2 = get_federated_metrics(\n",
    "    federated_results_2,\n",
    "    EXPERIMENT_REPEATS,\n",
    "    NUM_CLIENTS,\n",
    "    TOTAL_ROUNDS,\n",
    "    centralised_evaluation=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax_losses = axs[0]\n",
    "ax_losses.set_title(\"Training Loss\")\n",
    "ax_losses.set_ylabel(\"Average Loss\")\n",
    "ax_losses.set_xlabel(\"Round\")\n",
    "# ax_losses.set_prop_cycle(color=['red', 'orange', 'yellow', 'green', 'cyan', 'blue', 'violet'])\n",
    "\n",
    "plot_losses(ax=ax_losses,\n",
    "            xs=rounds,\n",
    "            losses=federated_losses,\n",
    "            label=\"TD3 FedAvg\",\n",
    "            color=\"green\")\n",
    "\n",
    "\n",
    "plot_losses(ax=ax_losses,\n",
    "            xs=rounds,\n",
    "            losses=federated_losses_2,\n",
    "            label=\"TD3 FedAvg (Actor)\",\n",
    "            color=\"blue\")\n",
    "\n",
    "# Evaluation Reward\n",
    "ax_rewards = axs[1]\n",
    "ax_rewards.set_title(\"Evaluation Reward\")\n",
    "ax_rewards.set_ylabel(\"Average Episode Reward\")\n",
    "ax_rewards.set_xlabel(\"Round\")\n",
    "\n",
    "plot_rewards(ax=ax_rewards,\n",
    "            xs=rounds,\n",
    "            rewards=federated_rewards,\n",
    "            label=\"TD3 FedAvg\",\n",
    "            color=\"green\")\n",
    "\n",
    "plot_rewards(ax=ax_rewards,\n",
    "            xs=rounds,\n",
    "            rewards=federated_rewards_2,\n",
    "            label=\"TD3 FedAvg (Actor)\",\n",
    "            color=\"blue\")\n",
    "\n",
    "handles, labels = ax_rewards.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
